{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2883347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use NLTK and SpaCy Python libraries for Natural Language Processing - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fcfecdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from nltk) (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43042480",
   "metadata": {},
   "source": [
    "### Tokenization Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a781137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello Friends, You are here in Accelerate'AI Careers YouTube Channel.\n",
    "Please do hands on practice with me in parallel!\n",
    "Please watch my entire playlist to become an expert in NLP and Gen AI.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e92787da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends, You are here in Accelerate'AI Careers YouTube Channel.\n",
      "Please do hands on practice with me in parallel!\n",
      "Please watch my entire playlist to become an expert in NLP and Gen AI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cedccd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization - convert paragraphs into sentences\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86efc65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11e1a256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello Friends, You are here in Accelerate'AI Careers YouTube Channel.\",\n",
       " 'Please do hands on practice with me in parallel!',\n",
       " 'Please watch my entire playlist to become an expert in NLP and Gen AI.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5a04038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1e0f641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends, You are here in Accelerate'AI Careers YouTube Channel.\n",
      "Please do hands on practice with me in parallel!\n",
      "Please watch my entire playlist to become an expert in NLP and Gen AI.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0161e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization - convert paragraph or sentences into words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8269c3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " ',',\n",
       " 'You',\n",
       " 'are',\n",
       " 'here',\n",
       " 'in',\n",
       " \"Accelerate'AI\",\n",
       " 'Careers',\n",
       " 'YouTube',\n",
       " 'Channel',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'hands',\n",
       " 'on',\n",
       " 'practice',\n",
       " 'with',\n",
       " 'me',\n",
       " 'in',\n",
       " 'parallel',\n",
       " '!',\n",
       " 'Please',\n",
       " 'watch',\n",
       " 'my',\n",
       " 'entire',\n",
       " 'playlist',\n",
       " 'to',\n",
       " 'become',\n",
       " 'an',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " 'and',\n",
       " 'Gen',\n",
       " 'AI',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0aaf5408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Friends', ',', 'You', 'are', 'here', 'in', \"Accelerate'AI\", 'Careers', 'YouTube', 'Channel', '.']\n",
      "['Please', 'do', 'hands', 'on', 'practice', 'with', 'me', 'in', 'parallel', '!']\n",
      "['Please', 'watch', 'my', 'entire', 'playlist', 'to', 'become', 'an', 'expert', 'in', 'NLP', 'and', 'Gen', 'AI', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54004739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7cbd10d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " ',',\n",
       " 'You',\n",
       " 'are',\n",
       " 'here',\n",
       " 'in',\n",
       " 'Accelerate',\n",
       " \"'\",\n",
       " 'AI',\n",
       " 'Careers',\n",
       " 'YouTube',\n",
       " 'Channel',\n",
       " '.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'hands',\n",
       " 'on',\n",
       " 'practice',\n",
       " 'with',\n",
       " 'me',\n",
       " 'in',\n",
       " 'parallel',\n",
       " '!',\n",
       " 'Please',\n",
       " 'watch',\n",
       " 'my',\n",
       " 'entire',\n",
       " 'playlist',\n",
       " 'to',\n",
       " 'become',\n",
       " 'an',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " 'and',\n",
       " 'Gen',\n",
       " 'AI',\n",
       " '.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93ce04af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b0a4839",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adb52119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " ',',\n",
       " 'You',\n",
       " 'are',\n",
       " 'here',\n",
       " 'in',\n",
       " \"Accelerate'AI\",\n",
       " 'Careers',\n",
       " 'YouTube',\n",
       " 'Channel.',\n",
       " 'Please',\n",
       " 'do',\n",
       " 'hands',\n",
       " 'on',\n",
       " 'practice',\n",
       " 'with',\n",
       " 'me',\n",
       " 'in',\n",
       " 'parallel',\n",
       " '!',\n",
       " 'Please',\n",
       " 'watch',\n",
       " 'my',\n",
       " 'entire',\n",
       " 'playlist',\n",
       " 'to',\n",
       " 'become',\n",
       " 'an',\n",
       " 'expert',\n",
       " 'in',\n",
       " 'NLP',\n",
       " 'and',\n",
       " 'Gen',\n",
       " 'AI',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701db6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
